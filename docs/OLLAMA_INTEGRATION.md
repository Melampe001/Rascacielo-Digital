# 游뱄 Ollama Integration

Integraci칩n de LLMs locales para an치lisis inteligente de c칩digo.

## Requisitos

- **RAM m칤nima:** 8GB (16GB recomendado)
- **Storage:** 10-50GB seg칰n modelos
- **GPU:** Opcional (CUDA/ROCm para mejor rendimiento)

## Instalaci칩n

### 1. Instalar Ollama

**macOS/Linux:**

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Descargar de https://ollama.ai/download

### 2. Iniciar Ollama

```bash
ollama serve
```

### 3. Instalar modelos

```bash
# Para an치lisis de c칩digo (13GB)
ollama pull codellama:13b

# Para prop칩sito general (4GB)
ollama pull mistral:7b

# Para an치lisis r치pido (4GB)
ollama pull codellama:7b
```

## Uso

### Con Python Master

```javascript
const { PythonMaster } = require('./agents/masters');

const master = new PythonMaster({
  useOllama: true,
  ollamaModel: 'codellama:13b'
});

// An치lisis profundo
const analysis = await master.analyze(code, { deep: true });
console.log(analysis.llmInsights);
```

### Generar c칩digo con AI

```javascript
const { PythonMaster } = require('./agents/masters');

const master = new PythonMaster({
  useOllama: true
});

// Generar c칩digo con AI
const scaffold = await master.scaffold('fastapi', {
  name: 'my-api',
  useAI: true
});
```

### Optimizar c칩digo

```javascript
const optimizedCode = await master.optimize(originalCode);
console.log(optimizedCode);
```

### Detectar vulnerabilidades

```javascript
const issues = await master.detectIssues(code);
console.log(issues);
```

## Modelos Recomendados

| Tarea              | Modelo           | RAM  | Descripci칩n                   |
| ------------------ | ---------------- | ---- | ----------------------------- |
| An치lisis c칩digo    | codellama:13b    | 16GB | Mejor precisi칩n               |
| An치lisis r치pido    | codellama:7b     | 8GB  | Balance velocidad/calidad     |
| Generaci칩n         | codellama:34b    | 32GB | Mejor generaci칩n              |
| Seguridad          | llama2:13b       | 16GB | An치lisis de vulnerabilidades  |
| Prop칩sito general  | mistral:7b       | 8GB  | Uso general                   |

## Performance

Con Ollama habilitado:

- An치lisis b치sico: ~100ms (sin cambios)
- An치lisis profundo: ~5-30s (seg칰n modelo)
- Generaci칩n scaffold: ~10-60s

## Troubleshooting

### Ollama no responde

```bash
# Verificar estado
ollama list

# Reiniciar
pkill ollama
ollama serve
```

### Memoria insuficiente

Usar modelos m치s peque침os:

```bash
ollama pull codellama:7b  # En lugar de :13b
```

### Error de conexi칩n

Verificar que Ollama est칠 corriendo:

```bash
# Verificar con el manager
npm run ollama:check

# O manualmente
curl http://localhost:11434/api/tags
```

## Configuraci칩n

### Variables de entorno

```bash
# En .env
OLLAMA_ENABLED=true
OLLAMA_URL=http://localhost:11434
```

### Configuraci칩n por c칩digo

```javascript
const master = new PythonMaster({
  useOllama: true,
  ollamaModel: 'codellama:13b',
  ollamaURL: 'http://localhost:11434'
});
```

## Docker Deployment

Para usar Ollama con Docker:

```bash
# Iniciar servicios
docker-compose -f docker-compose.ollama.yml up -d

# Ver logs
docker-compose -f docker-compose.ollama.yml logs -f

# Detener servicios
docker-compose -f docker-compose.ollama.yml down
```

## Comandos NPM

```bash
# Verificar estado de Ollama
npm run ollama:check

# Iniciar con Docker
npm run ollama:docker
```

## Ejemplos Avanzados

### An치lisis con contexto personalizado

```javascript
const analysis = await master.analyze(code, {
  deep: true,
  context: {
    framework: 'fastapi',
    version: '0.109.0',
    focus: 'security'
  }
});
```

### Scaffold con opciones avanzadas

```javascript
const project = await master.scaffold('fastapi', {
  name: 'my-advanced-api',
  useAI: true,
  features: ['auth', 'database', 'caching'],
  database: 'postgresql',
  auth: 'jwt'
});
```

## Limitaciones

- Requiere Ollama instalado y corriendo localmente
- El an치lisis profundo puede ser lento (5-30s)
- Los modelos grandes requieren mucha RAM
- No funciona sin conexi칩n a Ollama

## Fallback Behavior

Si Ollama no est치 disponible:

- El sistema contin칰a funcionando con an치lisis b치sico
- Se registra una advertencia en la consola
- No se lanzan errores que rompan la aplicaci칩n

## Referencias

- [Ollama Oficial](https://ollama.ai)
- [CodeLlama Model](https://ollama.ai/library/codellama)
- [Mistral Model](https://ollama.ai/library/mistral)
